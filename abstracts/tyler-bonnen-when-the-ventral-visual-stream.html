
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>When the ventral visual stream is not enough: A deep learning account of medial temporal lobe involvement in perception</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-26 21:15");</script>
            /
            Track 4
            /
            Traditional talk
        </h3>
        <h1>When the ventral visual stream is not enough: A deep learning account of medial temporal lobe involvement in perception</h1><h2>tyler bonnen</h2><h3>Daniel L.K. Yamins, Stanford Universiry; Anthony D. Wagner, Stanford University</h3><h2>Abstract</h1><p>The medial temporal lobe (MTL) supports a constellation of memory-related behaviors. Its involvement in perceptual processing, however, has been subject to an enduring debate. This debate centers on perirhinal cortex (PRC), an MTL structure at the apex of the ventral visual stream (VVS), beset by decades of seemingly inconsistent experimental outcomes. We suggest that these apparent inconsistencies can be resolved by situating experimental behavior in relation to perceptual processing supported by the VVS. To evaluate this claim we leverage a deep learning approach that approximates visual behaviors supported by the VVS. We first apply this approach retroactively, modeling 29 published concurrent visual discrimination experiments: Excluding misclassified stimuli, there is a striking correspondence between a computational proxy for the VVS and PRC-lesioned behavior. Conversely, PRC-intact subjects outperform this model and PRC-lesioned subjects. We corroborate these results using novel, high-throughput psychophysics experiments: PRC-intact participants outperform a linear readout of electrophysiological recordings from the macaque VVS. Finally, in silico experiments suggest PRC enables out-of-distribution visual behaviors at rapid timescales: VVS-like models achieve PRC-dependent accuracy, but only with extensive training, while failing to generalize to out-of-sample stimuli. By situating these lesion, electrophysiological, and behavioral results within a shared computational framework, our work resolves decades of seemingly inconsistent experimental findings surrounding PRC involvement in perception. In doing so, we have identified biologically constraints that may inform future models of human-level visual behaviors.</p>
    </body>
    </html>
    