
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Accumulation of Salient Perceptual Events Predicts Human Subjective Time</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-30 17:15");</script>
            /
            Track 7
            /
            Traditional talk
        </h3>
        <h1>Accumulation of Salient Perceptual Events Predicts Human Subjective Time</h1><h2>Maxine Sherman (she/her)</h2><h3>Zafeirios Fountas, Wellcome Centre for Human Neuroimaging, University College London, London, UK;  Anil K. Seth, Sackler Centre for Consciousness Science, University of Sussex, UK; Warrick Roseboom, Sackler Centre for Consciousness Science, University of Sussex, UK</h3><h2>Abstract</h1><p>Human experience of time exhibits systematic, context-dependent deviations from objective clock time. For example, time is experienced differently at work than on holiday. However, leading explanations of time perception are not well-equipped to explain these deviations. We propose that salient changes in sensory dynamics are the units from which we construct our subjective estimates of duration, such that epochs will feel longer when they contain more salient events. We tested this idea in a human model-based neuroimaging experiment with a fully pre-registered analysis pipeline(osf.io/ce9tp). Forty participants watched naturalistic, silent videos and estimated their duration while fMRI was acquired. We extracted moment-to-moment changes in visual cortex BOLD dynamics (“events”), and using computational modelling we show that accumulated events in visual cortex predicted duration biases reflecting human experience of time. We were unable to predict subjective time from two control regions irrelevant for perceptual classification of our silent videos - auditory and somatosensory cortex - despite being able to predict ‘clock time’ from all three areas. Therefore, we show that trial-by-trial subjective duration judgements can be reconstructed purely from human BOLD activity reflecting perceptual classification of the content for which time is being judged. Our results reveal that human subjective time is based on information arising during the processing of our dynamic sensory environment, providing a computational basis for an end-to-end account of time perception.</p>
    </body>
    </html>
    