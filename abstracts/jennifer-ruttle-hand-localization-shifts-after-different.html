
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Hand localization shifts after different types of visual error feedback in reaching</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-26 21:00");</script>
            /
            Track 7
            /
            Interactive talk
        </h3>
        <h1>Hand localization shifts after different types of visual error feedback in reaching</h1><h2>Jennifer Ruttle</h2><h3>Jennifer Ruttle, York University; Bernard M. 't Hart, York University; Denise Henriques, York University</h3><h2>Abstract</h1><p>Perturbed error feedback during reaching movements leads to adapted reaches, which involves at least two processes, as shown by a two-rate model (Smith et al., 2006). This model includes a fast and slow process, that respond differently to errors, and combine to change motor output, including those changes seen during rotated visual feedback training (McDougle et al., 2015). This model is specifically able to explain a rebound toward a previous performance when error feedback is removed. Adapting to rotated visual feedback also leads to shifted estimates of hand location, participants perceive their unseen hand as being closer to where they visually experienced the misaligned hand-cursor. Here we test how different types of feedback during training affect the rebound predicted by the two-rate model and shifts in proprioceptive-estimates of hand location. The feedback conditions include the typical continuous cursor feedback, and more “impoverished” conditions, terminal cursor feedback, and passive-exposure feedback. During rotated exposure feedback training, participants’ hands were moved by the robot with straight hand paths that perfectly countered any rotation, but with a normal error-clamp phase at the end. We find terminal cursor feedback produces a smaller rebound while the rebounds in the other conditions are indistinguishable. Regardless of the type of cursor feedback, perceived changes in hand location emerged very rapidly and with a similar time course throughout training across all three conditions. To further test the rapid changes in hand estimates, another group experienced a visuomotor rotation that changes every 12 trials. Participants attempted to learn each rotation with proprioceptive hand estimates again shifting quite rapidly following each change in rotation direction and size. However, the size of shift decrease slightly as training progressed, indicating a possible change in error sensitivity across time. These results underline the robustness and speed by which vision recalibrates proprioception. </p>
    </body>
    </html>
    