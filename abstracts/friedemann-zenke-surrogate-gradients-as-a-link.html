
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Surrogate gradients as a link between connectivity and computational function in spiking neural networks</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-30 08:30");</script>
            /
            Track 1
            /
            Traditional talk
        </h3>
        <h1>Surrogate gradients as a link between connectivity and computational function in spiking neural networks</h1><h2>Friedemann Zenke</h2><h3>Friedemann Zenke, Friedrich Miescher Institute</h3><h2>Abstract</h1><p>The brain relies on intricately connected spiking neural networks to solve complex information processing tasks. To understand the principles of this processing, models are indispensable. However, modeling spiking neural networks that mimic a similar computational complexity in-silico has remained challenging due to a lack of theoretical understanding of how neuronal connectivity translates into network-level function. In deep learning, gradient-based algorithms established this link by optimizing objective functions. However, gradient-based methods usually fail in spiking neural networks due to the non-differential nature of spiking neuron models, which precludes gradient computation.
Surrogate gradient learning has emerged as a versatile solution to this problem, allowing us to build functional multi-layer and recurrent spiking neural networks capable of solving diverse computational tasks. Here, I will first motivate and introduce surrogate gradient learning and illustrate how biologically plausible voltage-based three-factor learning rules emerge naturally within this framework. Second, I will showcase the flexibility and remarkable robustness of surrogate gradients on several spike-based information processing problems. Finally, I will show how surrogate gradients allow training spiking neural networks implemented in dedicated neuromorphic hardware, thereby enabling unprecedented processing speed and power-efficiency.
</p>
    </body>
    </html>
    