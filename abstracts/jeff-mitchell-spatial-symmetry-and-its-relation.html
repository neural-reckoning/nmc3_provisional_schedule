
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Spatial Symmetry and its Relation to Grammatical Structures</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-27 18:15");</script>
            /
            Track 4
            /
            Traditional talk
        </h3>
        <h1>Spatial Symmetry and its Relation to Grammatical Structures</h1><h2>Jeff Mitchell</h2><h3>Jeffrey S. Bowers, University of Bristol</h3><h2>Abstract</h1><p>Although there has been substantial debate over the merits of neural versus symbolic models of cognition, both approaches face a similar fundamental challenge. In each case, the set of functions available to the modeller includes every computable function, and so the fundamental question of which subset of these functions is cognitively relevant still needs to be answered, whichever toolkit is chosen. Within the symbolic approach, the Chomsky hierarchy provides an important example of possible constraints on the computational power of a formal grammar, in relation to human linguistic behaviour. In contrast, the most important constraint employed within the field of deep learning, has been the translational symmetry of image processing as implemented in convolutional networks. 

Superficially at least, these appear to be two quite divergent means of limiting the space of functions, relating to the structure of space versus the syntax of natural languages. However, it is nonetheless possible to draw a connection between them. In particular, we show that imposing a convolutional structure on the memory cells of a recurrent network produces a stack like data structure, suitable for handling Context Free Grammars, the most common formalism applied to natural languages. Such an architecture reuses the symmetry under spatial translations employed within image processing applications as a method to obtain the uniformity across memory registers that is required for the same symbol to be represented at multiple locations. 

Thus, despite their differences, neural and symbolic researchers have found their attention drawn to spaces of functions that share a common ingredient, in terms of translation invariance across a set of equivalent positions. More abstractly, we argue that symmetry has an important role to play in delimiting sets of interesting functions, that goes beyond just spatial translations and convolution.</p>
    </body>
    </html>
    