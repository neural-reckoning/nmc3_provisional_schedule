
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Building a hierarchically controlled robot arm for real world behavior</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	};
    function time_between(start, end) {
        var s = moment.utc(start);
        var e = moment.utc(end);
        var now = moment();
        return (s<=now) && (now<=e);
    };
    function update_visibility() {
        var now = moment();
        var elems = document.getElementsByClassName("visible_at_time");
        for(var i=0; i<elems.length; i++) {
            s = moment.utc(elems[i].dataset.start);
            e = moment.utc(elems[i].dataset.end);
            if ( (s<=now) && (now<=e) ) {
                elems[i].style.display = "block";
            } else {
                elems[i].style.display = "none";
            }
        }
    };
    setInterval(update_visibility, 60*1000);
    
        </script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-28 13:00");</script>
            /
            Track 5
            /
            Interactive talk
            <div class="visible_at_time" data-start="2020-10-28 13:00" data-end="2020-10-28 13:15">
                <a href='https://www.youtube.com/watch?v=KZaeUw7vWZs'><i class="fa fa-youtube-play" style="font-size:24px;color:red"></i></a>
                <a href='https://www.youtube.com/watch?v=KZaeUw7vWZs'>Watch now on YouTube</a>
            </div>
        </h3>
        <h1>Building a hierarchically controlled robot arm for real world behavior</h1><h2>Adam Matic (he, him)</h2><h3>Alex Gomez-Marin</h3><h2>Abstract</h1><p>Motor control research often relies on computational modelling, aiming at plausible and biologically relevant results in simulated environments. In contrast, robotic approaches must work in the real world with delays, nonlinearities and noise - posing a greater challenge on the proposed architectures, and promising greater insight. Commercially available robots with pre-programmed control systems are an appropriate solution to perform certain tasks from the get-go. From a theoretical perspective, however, this can be limiting since alternative control architectures cannot be tested. Here we built a simple robot arm with two shoulder joints, elbow and wrist, from the ground up, 3D printed arm segments, DC motors as actuators; potentiometers, pressure and tilt sensors and a webcam for sensory input. We programmed the control architecture for visual and kinesthetic arm control on a PC and an Arduino-compatible microcontroller. The robot performed a planar reaching task and a tracking task following a continuously moving visual reference, while simultaneously maintaining constant pen pressure and angle of tilt. While modest in speed and precision, the robot arm successfully performed these tasks, showing biologically relevant movement features, such as bell-shaped speed profiles, isochronous rise times, and the emergence of the speed-curvature power law at high speeds. Next, we imposed movement constraints and perturbations such as inclining the writing surface, displacing the visual marker, blocking the wrist, and rotating the visual field. Without any learning algorithms or parameter adjustments, the robot performed successful reaching and tracking. In sum, while maintaining computational simplicity, the proposed architecture is capable of integrating several modalities of sensory information and coordinating multiple degrees of freedom in real-world tasks, demonstrating how some observed biological movement invariances might emerge not from a complex plan, but from the interaction of brain, body and environment</p>
        <script type="text/JavaScript">
            update_visibility();
        </script>
    </body>
    </html>
    