
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Perceptual and semantic boundaries create events in auditory streams</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-28 17:00");</script>
            /
            Track 8
            /
            Traditional talk
        </h3>
        <h1>Perceptual and semantic boundaries create events in auditory streams</h1><h2>Omri Raccah</h2><h3>Keith Doelling, Institut de l'Audition, Institut Pasteur, Paris, France; Lila Davachi, Department of Psychology, Columbia University, New York, NY, USA; David Poeppel, Department of Psychology, New York University, New York, NY, USA</h3><h2>Abstract</h1><p>While our perceptual experience unfolds seemingly continuously over time, episodic memory preserves distinct events for storage and recollection. Therefore, a critical question is how the brain segments information into structured event representations. Previous work shows that contextual stability (in stimulus properties and goal-state) serves to temporally bind individual items into composite events; however, this has primarily been studied using visual and spatial memory paradigms. The current study adapts these paradigms in order to test the role of both perceptual and semantic contexts for event segmentation of auditory streams. Our results replicate the findings in other sensory modalities â€“ finding greater within-event temporal memory for items within perceptually bounded events. Furthermore, we show that naturally adopted mnemonic strategies during encoding selectively drive temporal memory. Finally, our analyses reveal that semantic boundaries strongly drive temporal binding effects as a function of semantic similarity within-events and across-event categorical transitions. These findings provide evidence that contextual stability may drive temporal binding in a modality independent manner and supports a role for non-sensory mechanisms in event segmentation. </p>
    </body>
    </html>
    