
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Weak signatures of audiovisual integration in mouse visual cortex</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-29 17:00");</script>
            /
            Track 2
            /
            Interactive talk
        </h3>
        <h1>Weak signatures of audiovisual integration in mouse visual cortex</h1><h2>CÃ©lian Bimbard</h2><h3>Timothy Sit, UCL; Anna Lebedeva, UCL; Philip Coen, UCL; Kenneth Harris, UCL; Matteo Carandini, UCL</h3><h2>Abstract</h1><p>Sensory cortices are increasingly thought to encode multisensory information. For instance, mouse primary visual cortex appears to be influenced by auditory inputs, which have been suggested to provide global inhibition or loudness and frequency specific information. However, sounds can also evoke behavioral responses. Such behavioral responses are now known to elicit neural activity across the brain. Thus, a potential confound between acoustic-related and movement-related activity could exist in multisensory studies. 

To investigate this issue, we measured the neuronal responses to natural movies and natural sounds in primary visual cortex (V1) using chronically implanted Neuropixels probes, thus recording the activity of hundreds of units.

First, we replicated previous results showing that sounds could evoke neural activity in V1. Sound-evoked responses were weak compared to video-evoked responses, but we could still significantly decode sound identity from population activity. Second, we observe that mice showed stereotypical, unsolicited behaviors in response to the sounds (from startling to more complex behaviors). Third, we show that removing movement-related activity from V1 population activity drastically reduced the responses to sounds. We conclude that a significant part of sound-evoked responses in V1 comes from indirect behavioral responses. Analysis of  changes in behavioral states is thus required for a careful characterization of auditory inputs to other sensory areas.
</p>
    </body>
    </html>
    