
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Ultra-sparse spike sequences for automatic recognition of auditory objects</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	};
    function time_between(start, end) {
        var s = moment.utc(start);
        var e = moment.utc(end);
        var now = moment();
        return (s<=now) && (now<=e);
    };
    function update_visibility() {
        var now = moment();
        var elems = document.getElementsByClassName("visible_at_time");
        for(var i=0; i<elems.length; i++) {
            s = moment.utc(elems[i].dataset.start);
            e = moment.utc(elems[i].dataset.end);
            if ( (s<=now) && (now<=e) ) {
                elems[i].style.display = "block";
            } else {
                elems[i].style.display = "none";
            }
        }
    };
    setInterval(update_visibility, 60*1000);
    
        </script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-29 01:15");</script>
            /
            Track 2
            /
            Traditional talk
            <div class="visible_at_time" data-start="2020-10-29 01:15" data-end="2020-10-29 01:30">
                <a href='https://www.youtube.com/watch?v=hYBrHmw4tuU'><i class="fa fa-youtube-play" style="font-size:24px;color:red"></i></a>
                <a href='https://www.youtube.com/watch?v=hYBrHmw4tuU'>Watch now on YouTube</a>
            </div>
        </h3>
        <h1>Ultra-sparse spike sequences for automatic recognition of auditory objects</h1><h2>Leonardo Tavares</h2><h3>Dezhe Jin, Pennsylvania State University</h3><h2>Abstract</h1><p>In the last few years, Deep Learning has tremendously improved the performance of automatic speech recognition (ASR) systems. It has also been applied to related tasks, such as birdsong syllable recognition [2]. However, this success comes at the expense of large amounts of training data and computational power. When data is scarce, the performance tends to decrease rapidly. This hinders applications where high accuracy is required from a limited amount of labeled exemplars.<br/><br/>Here we explore a radically different approach to the task of auditory object recognition from continuous sound streams. Our approach is inspired by how the auditory system represents auditory objects. Namely, that neurons in high auditory areas fire sparsely and in patterns that correlate with specific auditory objects. In addition, these patterns are also seen to be robust against noise.<br/><br/>We train feature-detecting neurons that respond to acoustic structures within short-time windows of about 50 ms. Together, these neurons transform the sound input into a continuous stream of spikes. This spike representation is ultra-sparse and precisely timed. Auditory objects consisting of well defined spectrotemporal structures are then stored as templates of spike sequences. During testing, a template matching algorithm performs the search, discovery and classification of spike sequences in the continuous sound input.<br/><br/>Our contribution is two-fold. First, we demonstrate our approach in the task of segmenting and classifying Bengalese finch syllables from continuous birdsong recordings. We show that our system generalizes well even when only a few syllables are available for training. Second, we make available to the community a new open-source software for audio data annotation, which we call BASS. Our software is designed to allow users to quickly mark, compare and classify segments extracted from spectrograms. We find it particularly useful for manually annotating large sets of birdsong data.<br/><br/>[1] Tavares and Jin. Ultra-sparse spike sequences for automatic recognition of auditory objects. In preparation.<br/>[2] Y. Cohen et. al. 2020. TweetyNet: A neural network that enables high-throughput, automated annotation of birdsong. bioRxiv doi: https://doi.org/10.1101/2020.08.28.272088 </p>
        <script type="text/JavaScript">
            update_visibility();
        </script>
    </body>
    </html>
    