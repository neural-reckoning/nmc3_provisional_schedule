
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Behavioral models of visual working memory do not generalize to the whole-report paradigm</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-29 16:30");</script>
            /
            Track 5
            /
            Traditional talk
        </h3>
        <h1>Behavioral models of visual working memory do not generalize to the whole-report paradigm</h1><h2>Ben Cuthbert (he/him)</h2><h3>Dominic Standage, Queen's University, Martin Par√©, Queen's University, Gunnar Blohm, Queen's University</h3><h2>Abstract</h1><p>Many prominent behavioral models of visual working memory have been both motivated and validated by results from "single-report" change-detection and delayed estimation tasks. Recently, a new "whole-report" paradigm was introduced to address shortcomings with these canonical tasks, and to support claims about fundamental properties of visual working memory. However, it is unclear how well results from the relatively complex whole-report paradigm generalize to simpler tasks (and vice versa). Here, we use an existing whole-report dataset to show that participant responses exhibit statistical dependencies between simultaneously-presented stimuli that cannot be accounted for by an entire class of models that assumes independent encoding. We then fit several hierarchical encoding models (proposed to capture dependencies between stimuli) to the data, and find that this model class is also unable to account for whole-report behavior. Finally, we compare response distributions between stimulus types, and find that the dependencies exhibited when reporting hue are not present when reporting orientation, despite very similar overall task performance. Rather than proposing a model to capture behavior across all task paradigms, we suggest that this failure to generalize is the result of model mis-specification. We argue that some visual working memory models would be better conceptualized as models of change-detection or delayed estimation performance, and that more care should be taken when integrating results across stimulus modalities and behavioral paradigms into a single, unified theory.</p>
    </body>
    </html>
    