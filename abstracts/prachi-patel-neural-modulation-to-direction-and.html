
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Neural modulation to direction and speaker in spatial multi-talker speech perception</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	};
    function time_between(start, end) {
        var s = moment.utc(start);
        var e = moment.utc(end);
        var now = moment();
        return (s<=now) && (now<=e);
    };
    function update_visibility() {
        var now = moment();
        var elems = document.getElementsByClassName("visible_at_time");
        for(var i=0; i<elems.length; i++) {
            s = moment.utc(elems[i].dataset.start);
            e = moment.utc(elems[i].dataset.end);
            if ( (s<=now) && (now<=e) ) {
                elems[i].style.display = "block";
            } else {
                elems[i].style.display = "none";
            }
        }
    };
    setInterval(update_visibility, 60*1000);
    
        </script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-28 17:15");</script>
            /
            Track 7
            /
            Traditional talk
            <div class="visible_at_time" data-start="2020-10-28 17:15" data-end="2020-10-28 17:30">
                <a href='https://www.youtube.com/watch?v=k0C7mrbBiy8'><i class="fa fa-youtube-play" style="font-size:24px;color:red"></i></a>
                <a href='https://www.youtube.com/watch?v=k0C7mrbBiy8'>Watch now on YouTube</a>
            </div>
        </h3>
        <h1>Neural modulation to direction and speaker in spatial multi-talker speech perception</h1><h2>Prachi Patel</h2><h3>Stephan Bickel,Hofstra Northwell School of Medicine; Jose L. Herrero,Hofstra Northwell School of Medicine; Ashesh D. Mehta,Hofstra Northwell School of Medicine; Nima Mesgarani, Columbia University</h3><h2>Abstract</h1><p>Humans can easily attend to the speech of a single talker in a mixture when the talkers are spatially separated. However, the neural mechanisms in the human brain that enable this ability are unclear. Specifically, it is not known how spatial separation affects the neural representation of the speech mixture, and how attention to either the location or the identity of a talker modulates the cortical response. We recorded intracranially from the human auditory cortex while subjects listened to spatial talkers either presented stand alone or in a mixture. We demonstrate that the neural encoding of phonemes is unaffected by the location of a talker presented in isolation, but narrows to the speech of the contralateral talker when presented in a mixture. Further, we show that attention to the spatial location and the talker identity have differential neural effects: attention to location modulates the overall baseline of neural response level while attention to talker identity modulates spectrotemporal selectivity.  Our results reveal how the human auditory cortex adjusts its neural tuning to enable listeners to attend to a talker in a spatially separated mixture, with implications for more accurate neurophysiological models of speech processing.</p>
        <script type="text/JavaScript">
            update_visibility();
        </script>
    </body>
    </html>
    