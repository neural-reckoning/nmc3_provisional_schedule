
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Electrophysiology of Affordances</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	};
    function time_between(start, end) {
        var s = moment.utc(start);
        var e = moment.utc(end);
        var now = moment();
        return (s<=now) && (now<=e);
    };
    function update_visibility() {
        var now = moment();
        var elems = document.getElementsByClassName("visible_at_time");
        for(var i=0; i<elems.length; i++) {
            s = moment.utc(elems[i].dataset.start);
            e = moment.utc(elems[i].dataset.end);
            if ( (s<=now) && (now<=e) ) {
                elems[i].style.display = "block";
            } else {
                elems[i].style.display = "none";
            }
        }
    };
    setInterval(update_visibility, 60*1000);
    
        </script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-30 08:30");</script>
            /
            Track 4
            /
            Traditional talk
            <div class="visible_at_time" data-start="2020-10-30 08:30" data-end="2020-10-30 08:45">
                <a href='https://www.youtube.com/watch?v=DA7vgHrT3OU'><i class="fa fa-youtube-play" style="font-size:24px;color:red"></i></a>
                <a href='https://www.youtube.com/watch?v=DA7vgHrT3OU'>Watch now on YouTube</a>
            </div>
        </h3>
        <h1>Electrophysiology of Affordances</h1><h2>Karthik Mahesh Varadarajan</h2><h2>Abstract</h1><p>Affordances, in scientific literature have largely been treated either as psychophysical action schema or as visual percept abstractions. The neurobiology of affordances is not well-understood. In this paper, we attempt to explain the neurobiology of affordances through a simplified observational model, defined in terms of electrophysiology. In particular, we analyze affordances using cerebral electrophysiology measurements, namely electroencephalography (EEG) and arm muscular electrophysiology measurements, namely electromyography (EMG). Fourier and Wavelet based analysis of EEG measurements from 5 participants during 12 affordance executions across three modes - observational, simulated and ego-execution demonstrate two key effects. Firstly, these sensorimotor EEG affordance codes are distinct and uniquely discriminant across the affordance categories. Secondly, the affordance codes show a high-degree of similarity across the three execution modes, indicating the role of mirror neuronal behavior in affordance processing in the brain. On the other hand, EMG measurements across 33 different grasp affordance executions (as defined by KTH grasp taxonomy), show a high degree of overlap, especially across precision pinch grasp types. This indicates that EMG based motor codes may be insufficient in modeling  sensorimotor affordance codes and the wider diversity of EEG codes is likely to be necessary in successful characterization.</p>
        <script type="text/JavaScript">
            update_visibility();
        </script>
    </body>
    </html>
    