
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Neural encoding of phonemes modulated by linguistic information</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	};
    function time_between(start, end) {
        var s = moment.utc(start);
        var e = moment.utc(end);
        var now = moment();
        return (s<=now) && (now<=e);
    };
    function update_visibility() {
        var now = moment();
        var elems = document.getElementsByClassName("visible_at_time");
        for(var i=0; i<elems.length; i++) {
            s = moment.utc(elems[i].dataset.start);
            e = moment.utc(elems[i].dataset.end);
            if ( (s<=now) && (now<=e) ) {
                elems[i].style.display = "block";
            } else {
                elems[i].style.display = "none";
            }
        }
    };
    setInterval(update_visibility, 60*1000);
    
        </script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-28 17:00");</script>
            /
            Track 7
            /
            Traditional talk
            <div class="visible_at_time" data-start="2020-10-28 17:00" data-end="2020-10-28 17:15">
                <a href='https://www.youtube.com/watch?v=1gAtVTbcfiQ'><i class="fa fa-youtube-play" style="font-size:24px;color:red"></i></a>
                <a href='https://www.youtube.com/watch?v=1gAtVTbcfiQ'>Watch now on YouTube</a>
            </div>
        </h3>
        <h1>Neural encoding of phonemes modulated by linguistic information</h1><h2>Seung-Goo Kim</h2><h3>Federico de Martino, University of Maastricht; Tobias Overath, Duke University</h3><h2>Abstract</h1><p>Speech perception entails the mapping of the acoustic waveform to linguistic representations. For this acousto-linguistic transformation to succeed, the speech signal needs to be tracked over various temporal windows ranging from phonemes (tens of milliseconds) to sentences (seconds). Using a sound quilting algorithm (Overath et al., 2015) to control the temporal extent of speech structure, we recently showed that an acoustic analysis of temporal speech structure occurs in superior temporal sulcus (STS), while left inferior frontal gyrus (IFG) is engaged in mapping temporal speech structure to linguistic representations (Overath & Paik, under review).<br/>Here, we provide novel evidence that the neural encoding of a fundamental linguistic unit (phoneme) is modulated by both the acoustic and linguistic contexts using accelerated fMRI (TR = 1.2 s). Ten native English speakers without knowledge of Korean listened to 33-s long speech sounds (either phoneme-quilted speech, or natural speech) in native (English) or foreign (Korean) languages. A linearized encoding analysis was performed using ridge regression with finite-impulse response models. The performance of various models either encoding certain conditions/predictors or not was compared in terms of Pearson correlation coefficient. Group-level statistical inference used a cluster-based permutation test.<br/>Durations of phoneme classes (linguistic features) in addition to envelope (acoustic features) improved the prediction of BOLD signal in the Heschlâ€™s gyrus (HG), planum temporale (PT), superior temporal gyrus (STG), and posterior superior temporal sulcus (STS), bilaterally (cluster-p = 0.001). Critically, encoding language (English or Korean) or manipulation of speech (quilted or natural) improved the prediction of BOLD signal in the left STS from the anterior to the posterior (cluster-p < 0.007).<br/>These findings demonstrate that the information of rapid speech sounds embedded in the sluggish BOLD signal can be reliably extracted using linear regression. Furthermore, they suggest that acousto-linguistic transformation is modulated by linguistic processing.<br/></p>
        <script type="text/JavaScript">
            update_visibility();
        </script>
    </body>
    </html>
    