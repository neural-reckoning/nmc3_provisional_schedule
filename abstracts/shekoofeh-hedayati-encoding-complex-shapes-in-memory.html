
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Encoding Complex Shapes in Memory: The VAE_BP Model of Working Memory</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	};
    function time_between(start, end) {
        var s = moment.utc(start);
        var e = moment.utc(end);
        var now = moment();
        return (s<=now) && (now<=e);
    };
    function update_visibility() {
        var now = moment();
        var elems = document.getElementsByClassName("visible_at_time");
        for(var i=0; i<elems.length; i++) {
            s = moment.utc(elems[i].dataset.start);
            e = moment.utc(elems[i].dataset.end);
            if ( (s<=now) && (now<=e) ) {
                elems[i].style.display = "block";
            } else {
                elems[i].style.display = "none";
            }
        }
    };
    setInterval(update_visibility, 60*1000);
    
        </script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-30 21:15");</script>
            /
            Track 2
            /
            Traditional talk
            <div class="visible_at_time" data-start="2020-10-30 21:15" data-end="2020-10-30 21:30">
                <a href='https://www.youtube.com/watch?v=GBd2goR_WLU'><i class="fa fa-youtube-play" style="font-size:24px;color:red"></i></a>
                <a href='https://www.youtube.com/watch?v=GBd2goR_WLU'>Watch now on YouTube</a>
            </div>
        </h3>
        <h1>Encoding Complex Shapes in Memory: The VAE_BP Model of Working Memory</h1><h2>Shekoofeh Hedayati</h2><h3>Brad Wyble</h3><h2>Abstract</h1><p>Current computational models of working memory (WM) lack the ability to account for how memory communicates with visual semantics, therefore these models do not allow for how familiarity impact encoding. We created a model that combines a variational autoencoder (VAE; An & Cho, 2015) with the Binding Pool (BP) model (Swan & Wyble 2014) of WM and had it learn a colorized version of MNIST (LeCun et al.,1998). This new VAE-BP model shows how WM encoding and retrieval is mediated through visual semantics. Additionally, The shared resource pool of neurons in the model could store varying degrees of information about the specific shape or color of a single item, as well as its categorical label.<br/>A VAE structure was used with layers of input =28x28x3, L1=512, L2=256, L3= 4 + 4, L4= 256, L5=512, output=28x28x3 neurons respectively. The latent space (L3) was composed of two distinct maps for representing shape and color separately to be able to apply top-down control on itemsâ€™ features. The shape and color maps activations were combined at L4 and then used to reconstruct the input image.  <br/>After we colorized the MNIST images using 10 distinct colors with minor random variations, we trained the VAE with two separate objective functions for the shape and the color such that only one loss function was active for each batch. Four radial basis function kernel SVMs were applied to the color and shape maps of 10 trained models to test whether shape and color were partially disentangled by the two objective functions. As expected, representations in the shape map produced more accurate classification of shape than color on average (88.6% vs. 12.8%) and vice versa (95.8% vs. 18.1%).<br/>To represent complex patterns of digits in memory, a BP layer with 100 neurons was bidirectionally connected to L1, L2 or the latent space (VAE_BP). The results demonstrated the VAE_BP ability in robust classification accuracy of the shape and the color maps (76.8% and 84.6%). Applying the BP transformation to the L1 and L2 instead of the latent space, resulted in 25% and 38.1% shape accuracy and 40% and 44.2% color accuracy respectively. In conclusion, weights on the dimensions allowed the BP to store different levels of precision for shape and color which affects both the quality of a reconstructed image and the level of classification accuracy from the latent space. Moreover, the WM model is able to store and reconstruct novel patterns (e.g. Fashion MNIST images when training was only done on digit MNIST), but is more efficient in storing familiar information.<br/></p>
        <script type="text/JavaScript">
            update_visibility();
        </script>
    </body>
    </html>
    