
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Decoding Neural Activity into Language</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-29 01:00");</script>
            /
            Track 3
            /
            Interactive talk
        </h3>
        <h1>Decoding Neural Activity into Language</h1><h2>伟 黄</h2><h3>Wei Huang, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, 610054, PR China; Hongmei Yan, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, 610054, PR China; Kaiwen Cheng, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, 610054, PR China; Chong Wang, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, 610054, PR China; Jiyi Li, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, 610054, PR China; Zhentao Zuo, State Key Laboratory of Brain and Cognitive Science, Beijing MR Center for Brain Research, Institute of Biophysics, Chinese Academy of Sciences, Beijing 100101, China; Chen Li, Department of Medical Information Engineering, Sichuan University, Chengdu 610065, China; Huafu Chen, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu, 610054, PR China;</h3><h2>Abstract</h1><p>Transforming neural activities into language is revolutionary for interpreting the formation of language from visual perception. If the neural signal of visual activities is regarded as a special kind of human biological language, it is quite feasible that we can apply the machine translation architecture in Artificial Intelligence (AI) to decode those neural signals. Therefore, a progressive transfer language decoding model was proposed to decode visual fMRI signals into phrases or sentences when natural images were being processed. The results of a similarity analysis showed that human natural language was successfully derived from visual neural activities. Additionally, by comparing the decoding effectivity of different visual areas in space, we found that higher visual areas usually had better performance. At the same time, we found that the contribution curve of visual response patterns in language decoding rose first and then gradually decreased at successively different time points. Our findings proved that the neural representations elicited in the visual cortex when visual scene tasks were being performed already contained semantic information that could be utilized to generate the human natural language. Our study will help in the application of language-based brain-machine interfaces, such as assisting aphasiacs in normal communication with others.</p>
    </body>
    </html>
    