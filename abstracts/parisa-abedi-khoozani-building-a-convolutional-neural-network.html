
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Building a convolutional neural network to study the combination of allocentric and egocentric information</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	};
    function time_between(start, end) {
        var s = moment.utc(start);
        var e = moment.utc(end);
        var now = moment();
        return (s<=now) && (now<=e);
    };
    function update_visibility() {
        var now = moment();
        var elems = document.getElementsByClassName("visible_at_time");
        for(var i=0; i<elems.length; i++) {
            s = moment.utc(elems[i].dataset.start);
            e = moment.utc(elems[i].dataset.end);
            if ( (s<=now) && (now<=e) ) {
                elems[i].style.display = "block";
            } else {
                elems[i].style.display = "none";
            }
        }
    };
    setInterval(update_visibility, 60*1000);
    
        </script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-28 13:15");</script>
            /
            Track 4
            /
            Interactive talk
            <div class="visible_at_time" data-start="2020-10-28 13:15" data-end="2020-10-28 13:30">
                <a href='https://www.youtube.com/watch?v=DA7vgHrT3OU'><i class="fa fa-youtube-play" style="font-size:24px;color:red"></i></a>
                <a href='https://www.youtube.com/watch?v=DA7vgHrT3OU'>Watch now on YouTube</a>
            </div>
        </h3>
        <h1>Building a convolutional neural network to study the combination of allocentric and egocentric information</h1><h2>Parisa Abedi Khoozani</h2><h3>Richard Wildes; Douglas Crawford</h3><h2>Abstract</h1><p>Both behavioural (e.g. Byrne et. al, 2010; Klinghammer et. al, 2017) and physiology (e.g. Bharmauria) studies have shown that allocentric and egocentric information are combined for goal-directed movements toward visual targets. Nevertheless, the intrinsic coordinate representation and underlying processes for this combination remain a puzzle, mainly due to inadequacy of current theoretical models to explain data at different levels (i.e. behaviour, single neuron, or distributed networks). In response, we aim to build a theoretical framework to tackle this challenge. In particular, we propose a physiologically inspired neural network with two major components. First, a Convolutional Neural Network (CNN) to extract the allocentric and target information: Our CNN performs repeated convolution, rectifications, and normalization to first extract low level features from input images. With the features extracted, the challenge is to combine them to generate an abstract representation (allocentric cues and target position). We address this challenge by training a feature pooling layer at the end of our CNN network. Second, a Multi-Layer Perceptron network (MLP), which combines the allocentric information, here extracted feature maps from the CNN, with egocentric information, here initial gaze position, and generates the final gaze position: Our MLP consists of three fully connected hidden layers. These three layers incrementally transform the separately input allocentric and egocentric representations into an integrated motor response. Finally, following the MLP, an additional layer transforms motor responses into final gaze positions. We were able to train these physiologically-inspired networks to achieve good correspondence with test dataset (MLP: R2 = 0.99). These results suggest that our framework provides a suitable tool to study the underlying mechanisms of allocentric and egocentric integration.<br/>We would like to thank Vista for funding this research. </p>
        <script type="text/JavaScript">
            update_visibility();
        </script>
    </body>
    </html>
    