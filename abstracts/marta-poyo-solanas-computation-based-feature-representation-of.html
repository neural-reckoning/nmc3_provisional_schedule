
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Computation-Based Feature Representation of Body Expressions in the Human Brain</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-26 16:15");</script>
            /
            Track 4
            /
            Interactive talk
        </h3>
        <h1>Computation-Based Feature Representation of Body Expressions in the Human Brain</h1><h2>Marta Poyo Solanas</h2><h3>Marta Poyo Solanas, Maastricht University; Maarten Vaessen, Maastricht University; Beatrice de Gelder, Maastricht University.</h3><h2>Abstract</h1><p>Research on body category representation, action and emotion has provided evidence that the brain is endowed with social perception skills. Yet, the actual visual processes that drive expression perception are still unknown. We went beyond research on high-level emotion representations to discover which mid-level body features drive automatic emotion perception. 
We computed quantitative body features from angry, happy, fearful and neutral whole-body movements. These features represented kinematic (i.e. velocity, acceleration and vertical movement) and postural (i.e. symmetry, surface, the angles between body segments and limb contraction) aspects of affective body movements. Using representational similarity and multivoxel pattern analysis techniques, we investigated whether the (dis)similarity of body posture and kinematics between different emotional categories could explain neural responses to body expressions in and beyond body-selective regions. 
Our results reveal six major findings. First, computationally defined features are systematically related to distributed brain areas. Second, postural rather than kinematic features reflect the affective category structure of body movements. Limb angles and limb contraction were particularly relevant for distinguishing fear from other body expressions and were represented in several regions including affective, action observation and motor preparation networks. Third, the posterior superior temporal sulcus differentiated fearful from other affective categories using limb contraction rather than kinematics, despite this area being known for its involvement in biological motion processing. Fourth, extrastriate and fusiform body area (EBA and FBA) also showed greater tuning to postural features. Although the pattern of feature representation in these areas was similar, the stimuli representation in EBA was very dissimilar to that of FBA, possibly reflecting their different roles in body processing. Fifth, kinematic and postural feature processing was not segregated into dorsal and ventral streams, with the exception of one feature: velocity. Finally, the brain representation of emotional categories showed a distributed pattern. 
</p>
    </body>
    </html>
    