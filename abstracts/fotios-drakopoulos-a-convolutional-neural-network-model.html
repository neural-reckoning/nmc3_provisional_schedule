
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>A convolutional neural-network model of the human inner-hair-cell and auditory-nerve-fiber complex</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	};
    function time_between(start, end) {
        var s = moment.utc(start);
        var e = moment.utc(end);
        var now = moment();
        return (s<=now) && (now<=e);
    };
    function update_visibility() {
        var now = moment();
        var elems = document.getElementsByClassName("visible_at_time");
        for(var i=0; i<elems.length; i++) {
            s = moment.utc(elems[i].dataset.start);
            e = moment.utc(elems[i].dataset.end);
            if ( (s<=now) && (now<=e) ) {
                elems[i].style.display = "block";
            } else {
                elems[i].style.display = "none";
            }
        }
    };
    setInterval(update_visibility, 60*1000);
    
        </script>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-30 13:30");</script>
            /
            Track 7
            /
            Traditional talk
            <div class="visible_at_time" data-start="2020-10-30 13:30" data-end="2020-10-30 13:45">
                <a href='https://www.youtube.com/watch?v=1gAtVTbcfiQ'><i class="fa fa-youtube-play" style="font-size:24px;color:red"></i></a>
                <a href='https://www.youtube.com/watch?v=1gAtVTbcfiQ'>Watch now on YouTube</a>
            </div>
        </h3>
        <h1>A convolutional neural-network model of the human inner-hair-cell and auditory-nerve-fiber complex</h1><h2>Fotios Drakopoulos</h2><h3>Deepak Baby, Ghent University, Sarah Verhulst, Ghent University</h3><h2>Abstract</h1><p>Analytical descriptions of inner-hair-cell (IHC) and auditory-nerve-fiber (ANF) processing have evolved<br/>over the years, with IHC transduction models shifting from simplified low-pass filters to detailed<br/>conductance models which include basolateral outward K+ currents. State-of-the-art models of the IHC-<br/>ANF synapse complex describe the vibrations of the IHC stereocilia based on the mechanical drive to the<br/>IHC and model ANF spikes or instantaneous firing rates as resulting from the depletion and<br/>replenishment of different neurotransmitter stores. While such sensory models have progressed to<br/>accurately capture the nonlinear and dynamic properties of the neuronal processes associated with<br/>hearing, they typically comprise mechanistic descriptions and coupled sets of ordinary differential<br/>equations, rendering these models slow to compute.<br/>Here, we present a hybrid computational neuroscience - deep neural network (DNN) framework which<br/>simulates auditory IHC-ANF processing, to offer a fast and differentiable model which can be used in<br/>large-scale neuronal network models. Based on a state-of-the-art biophysical model of the auditory<br/>periphery (including human cochlear mechanics, IHC and ANF processing), we trained several<br/>convolutional neural network (CNN) architectures to learn the computations performed by the IHC-ANF<br/>complex (CoNNear IHC-ANF ). We determined the hyperparameters in these architectures (different layers,<br/>filter lengths, input and context windows, activation functions) on the basis of well-known single-unit<br/>IHC and ANF properties derived from experimental neuroscience studies. The model was trained using<br/>an acoustic speech corpus, and its performance evaluated using basic acoustic stimuli which were not<br/>included in the training set.<br/>The final CoNNear IHC-ANF model offers a 70-fold speed-up factor on a CPU and a 280-fold factor on a GPU<br/>when compared to the analytical IHC-ANF model processing computations. When connected to a DNN-<br/>based cochlear model, preferably CoNNear cochlea , population responses (e.g. CAP, ABR wave-I) can be<br/>simulated across a large number of cochlear tonotopic locations, and be used for backpropagation<br/>purposes. Our auditory periphery framework can be applied for the development of large-scale neural-<br/>network circuits aimed to advance our knowledge of unknown neuronal systems such as the brainstem<br/>and subcortical pathways, or the human auditory cortex.<br/><br/>Work supported by European Research Council ERC-StG-678120 (RobSpear)</p>
        <script type="text/JavaScript">
            update_visibility();
        </script>
    </body>
    </html>
    