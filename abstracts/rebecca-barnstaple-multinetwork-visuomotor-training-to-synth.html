
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>MULTINETWORK VISUOMOTOR TRAINING TO SYNTH-MUSIC INVESTIGATED WITH MOBILE EEG</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-26 21:15");</script>
            /
            Track 6
            /
            Traditional talk
        </h3>
        <h1>MULTINETWORK VISUOMOTOR TRAINING TO SYNTH-MUSIC INVESTIGATED WITH MOBILE EEG</h1><h2>Rebecca Barnstaple</h2><h3>Joseph FX DeSouza, York University; Lydia Jaufmann, Sein Jeung, Janna Protzak, Klaus Gramann, TU Berlin</h3><h2>Abstract</h1><p>We investigated the contribution of multinetwork training with music to neurorehabilitation. Previous studies showed that dance-based learning over 8-months produced BOLD signal changes in SMA using fMRI (Bar & DeSouza 2016) and resting state alpha power increases in frontal cortex post-dance in people with Parkinson’s disease compared to controls (Levkov et al 2014). 
Our current project assesses the impact of motor learning while subjects (n=16) learn a 30-second choreography. Trials include watching VIDEO (4 times), watching LIVE performances of the choreography (6 times), moving with the teacher (LEARN; 6 to 20 times), imagining performing from a first-person perspective (IMAGINE; 6 times), and finally performing in space (PERFORM; 6 times).  Sessions were recorded at the Berlin Mobile Brain/Body Imaging lab (BeMoBIL) which allows for acquisition of motion capture data synced with continuous recording of wireless mobile EEG (Brain Products ActiCap; 128 electrodes) in a dedicated 150 m2 lab space Movements are measured using 10 HTC Vive trackers running on Steam VR; music was rated for valence pre/post learning. 
Preliminary results show a significant shift (p=.0009) in participants’ affective scoring of the music post-learning, suggesting that the experience of dance learning impacts perception of previously unfamiliar music. This is important, as music is a consistent factor in rehabilitation programs using dance; however, to this point, the effects of motor learning on affective music perception have not been studied in this context.  Additionally, all participants reached our target of 80% or higher accuracy in reproducing the movement sequence within 20 LEARN trials, with expertise correlating to fewer trials. Next steps include frequency analysis of EEG data pre/post and during learning, along with movement analysis to determine the contribution of motor learning to observed brain-based changes.
</p>
    </body>
    </html>
    