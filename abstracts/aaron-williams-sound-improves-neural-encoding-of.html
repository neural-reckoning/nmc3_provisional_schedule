
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Sound improves neural encoding of stimulus direction in mouse V1</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-28 17:00");</script>
            /
            Track 5
            /
            Traditional talk
        </h3>
        <h1>Sound improves neural encoding of stimulus direction in mouse V1</h1><h2>Aaron Williams</h2><h3>Aaron Williams; Dr. Maria Geffen</h3><h2>Abstract</h1><p>In the natural world, we integrate visual and auditory signals during behaviors such as navigation and communication. Auditory and visual inputs can modulate the perception of the complementary modality, but the neural correlates of audiovisual integration are not fully understood. In the visual cortex, auditory stimuli modulate light-evoked firing rates of individual neurons. Here, we investigated how auditory stimuli modulate other aspects of neural processing in addition to firing rate, and whether this results in improved neural encoding of the visual stimulus. We presented visual drifting gratings with and without simultaneous auditory white noise to awake mice while recording neuronal activity in the primary visual cortex (V1). Sound modulated the light-evoked activity of 70% of light-responsive neurons, the majority of which increased their activity in association with sound. These firing rate changes were accompanied by increased response duration and reduced response latency. Additionally, across contrast levels sound reduced the trial to trial variability of the light-evoked response. In individual neurons that were additionally direction-selective, we found that sound improved the discriminability of the preferred direction over all other directions. Furthermore, sound improved the neural populationâ€™s encoding of the drifting grating direction, especially at low to intermediate contrast levels. These results demonstrate that simultaneous auditory input enhances the light-evoked response magnitude and timing and decreases variability in individual neurons, resulting in improved stimulus encoding at the individual neuron and population level. These findings expand our knowledge of how multisensory processing is mediated at a neural level.</p>
    </body>
    </html>
    