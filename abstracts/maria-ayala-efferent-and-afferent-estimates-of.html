
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Efferent and afferent estimates of hand location do not optimally integrate</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-29 21:15");</script>
            /
            Track 4
            /
            Interactive talk
        </h3>
        <h1>Efferent and afferent estimates of hand location do not optimally integrate</h1><h2>Maria Ayala</h2><h3>Marius 't Hart, Denise Henriques</h3><h2>Abstract</h1><p>The central nervous system (CNS) can rely on at least two sources of information in order to localize limb position in the absence of vision: (1) proprioceptive afferents from receptors in the limb, and (2) an efference copy of the motor command used by the cerebellum to generate a prediction of sensory consequences. To produce a reliable, single estimate about the location of the limb, these two sources of information could be optimally integrated using Bayesian integration, or maximum likelihood estimation (MLE). While some work has been done to investigate whether the CNS optimally combines visual and proprioceptive cues, less is known for the integration of non-sensory signals for limb estimates (i.e. efferent signals) given the difficulty of isolating them from the ever-present proprioceptive afferent signals. Here, we use our new paradigm to isolate efferent-based estimates of hand position from proprioception, by having over 200 healthy participants, both younger and older adults (55+), localize their hand after actively reaching in a self-chosen direction (active localization), or after being passively moved by a robotic manipulandum (passive localization). We then investigated if estimates of hand location based on both efferent and afferent signals (active localization) are more precise than those based only on proprioception (passive localization). According to the MLE rule, or other straightforward Bayesian mechanisms, variance should be lower in the localization condition where two signals are combined (active localization; prediction and proprioception) and higher with only a single source of information (passive localization; proprioception only), but we found no evidence in support of this. Further, we looked at whether weaker proprioceptive priors lead to greater learning-induced shifts in hand localization as would be predicted by Bayesian (optimal) inference. But we did not find that noisier, highly variable estimates were more susceptible to larger proprioceptive recalibration following adaptation to a perturbation suggesting a non-Bayesian mechanism for integration. Together, these findings make it less likely that the CNS uses optimal integration to combine efferent-based predicted sensory consequences and proprioceptive afferents when estimating and updating the final limb position.</p>
    </body>
    </html>
    