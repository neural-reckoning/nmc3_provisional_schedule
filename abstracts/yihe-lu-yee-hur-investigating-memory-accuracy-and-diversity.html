
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Investigating memory accuracy and diversity of generative replay in a predictive coding neural network</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-30 13:00");</script>
            /
            Track 8
            /
            Interactive talk
        </h3>
        <h1>Investigating memory accuracy and diversity of generative replay in a predictive coding neural network</h1><h2>Yihe Lu (Yee-Hur)</h2><h3>Tamás Földes, Cardiff University; Penelope Lewis, Cardiff University</h3><h2>Abstract</h1><p>Human memory replay is crucial for memory consolidation and knowledge integration. Similarly, replay can be deployed in an artificial neural network (ANN) to overcome the catastrophic forgetting problem in continual learning. For decades, experience replay (ER) has been applied; an ANN revisits exact old episodes whenever learning a new one. However, ER is not biologically plausible, because it requires an external memory storage with an increasing capacity to stock all old episodes (Kali and Dayan, 2004, Nat. Neurosci.). More recently, generative replay (GR) has been proposed for ANNs capable of predictive coding (PC). When such an ANN learns a new episode, it is simultaneously trained with self-generated samples. Despite the efficiency and the biological plausibility of GR, we were concerned about its effectiveness and robustness, and we investigated the quality of GR in restricted Boltzmann machines (RBMs), typically trained by contrastive divergence, a PC-like algorithm. We found the replay quality poor in terms of both accuracy and diversity. The individual replays were unidentifiable as any learned episodes by basic similarity measurements. Assorting them by Bayesian classifiers, we found stronger memories were replayed more frequently than weaker ones. Surprisingly, we still managed to replicate the success of an RBM’s continual learning by GR (Mocanu et al., 2016, ArXiv). Our results agree with the claim by van de Ven et al. (2020, Nat. Commun.) that replays with low quality is good enough for preserving past memory traces. We further considered a scenario of GR with relatively small numbers of replays. We found that an RBM could not guarantee all past memories to be replayed even once when learning a new episode, which tended to cause catastrophic forgetting of weaker memories. We argue that the amount of replay is a major constraint of the memory capacity for an ANN in continual learning.</p>
    </body>
    </html>
    