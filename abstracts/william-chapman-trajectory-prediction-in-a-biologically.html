
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Trajectory Prediction in a Biologically Inspired Network with a Strong Inductive Bias</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-27 21:30");</script>
            /
            Track 6
            /
            Traditional talk
        </h3>
        <h1>Trajectory Prediction in a Biologically Inspired Network with a Strong Inductive Bias</h1><h2>William Chapman</h2><h3>G. William Chapman IV; Michael E. Hasselmo</h3><h2>Abstract</h1><p>Animals are able to track objects through a visual space and make predictions about future locations after relatively few observations, even when the dynamics governing the trajectory are relatively novel. In contrast, machine learning techniques typically generalize poorly when presented with dynamics which don't match the training data. Here we introduce a model architecture which attempts to bridge this gap, by incorporating principles of hierarchical predictive coding. 

The essential component of this model is a predictive module which takes in two sequential observations, and outputs a prediction of the next observation of that same state space. Internally, each module has four layers: two inputs, a transformation layer that computes the difference between subsequent observations, and a prediction layer which takes in the more recent observation and the transition layer. These modules can operate on observations either directly from the target space, or the internal state from another predictive module. When training a network out of these modules, learning is on a per-module level based on its predicted value compared to the next observed input. 

We tested generalization of the model by using two test datasets, and comparing to a standard LSTM. In-domain generalization was tested with datasets which matched the dynamics of the training set, but with unseen initial conditions. Out of domain generalization was tested using PDEs of the same order and dimensionality, but different parameterizations, as the training set. 

We found that the models perform approximately the same for in-domain generalization with teacher forcing. For out of domain data, the predictive module network accumulates error at approximately half the rate as the LSTM model. 

These results demonstrate how an appropriately structured network utilizing a single local loss-function can perform similarly to globally optimized models, while generalizing better.
</p>
    </body>
    </html>
    