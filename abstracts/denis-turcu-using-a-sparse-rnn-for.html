
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Using a Sparse RNN for dynamic binary classification</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-30 12:30");</script>
            /
            Track 4
            /
            Interactive talk
        </h3>
        <h1>Using a Sparse RNN for dynamic binary classification</h1><h2>Denis Turcu</h2><h3>Larry Abbott, Zuckerman Mind Brain and Behavior Institute, Columbia Unicersity, New York, NY</h3><h2>Abstract</h1><p>Feedforward network models performing classification tasks rely on highly convergent output units that collect the information passed on by preceding layers.  Although convergent output-unit like neurons may exist in some biological neural circuits, notably the cerebellum, neocortical circuits do not exhibit any obvious candidates for this role, instead they are highly recurrent.  We investigated whether a sparsely connected recurrent network could perform classification in a distributed manner without ever bringing all of the relevant information to a single convergence unit (for an alternative approach see Kushnir, Fusi; 2018).

Our model is based on a sparse RNN that performs the classification dynamically. Specifically, the interconnections of the RNN are trained to resonantly amplify the magnitude of responses to some external inputs but not others.  The amplified and non-amplified responses then form the basis for binary classification.  In this arrangement, the minimum number of synapses per neuron required to reach maximum memory capacity scales only logarithmically with network size. Despite highly sparse connectivity, learned strong recurrent connections allow input information to flow to every neuron of the RNN, providing the basis for the distributed computation.  Furthermore, training allows the model to act as an evidence accumulator. The accumulator robustly maintains its decision for significant periods of time after the input is turned off, although the undriven network stops accumulating information. We studied different training methods and found that back-propagation through time yields the best results.  Our model is robust to various types of noise and works with different activation and loss functions, although certain of these are optimal.  In addition, the RNN can be constructed with a split “excitation–inhibition” architecture with little reduction in performance.</p>
    </body>
    </html>
    