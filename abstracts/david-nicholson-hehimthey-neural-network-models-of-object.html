
    <!doctype html>

    <html lang="en">
    <head>
        <meta charset="utf-8">    
        <script type="text/JavaScript" src="https://MomentJS.com/downloads/moment.js"></script>
        <script type="text/JavaScript" src="https://momentjs.com/downloads/moment-timezone-with-data.min.js"></script>
        <title>Neural network models of object recognition can also account for visual search behavior</title>
        <style>
            
    * {
        font-family: "Trebuchet MS", Helvetica, sans-serif;
    }
    
        </style>
        <script type="text/JavaScript">
            
	function LT(t) {
        var m = moment.utc(t).tz(moment.tz.guess());
		document.write(m.format('MMMM Do YYYY, HH:mm z'));
	}
    
        </script>
    </head>
    <body>
        <h3>
            <a href="https://neuromatch.io">Neuromatch</a> 3 /
            <script type="text/JavaScript">LT("2020-10-27 16:15");</script>
            /
            Track 4
            /
            Interactive talk
        </h3>
        <h1>Neural network models of object recognition can also account for visual search behavior</h1><h2>David Nicholson (he/him/they)</h2><h3>Astrid Prinz, Emory University</h3><h2>Abstract</h1><p>What limits our ability to find an object we are looking for? There are two competing families of models: one explains attentional limitations during visual search in terms of a serial processing computation, the other attributes them to noisy parallel processing. These models predict search behavior for the simplified stimuli often used in experiments, but it remains unclear how to extend them to account for search of complex natural scenes. Models exist for search of natural scenes, but they do not predict whether a given scene will limit search accuracy. Here we test an alternate hypothesis that could explain limitations across stimuli types: visual search is limited by an "untangling" computation, proposed to underlie object recognition. To test this hypothesis, we ask whether models of object recognition account for visual search behavior. The current best-in-class models are artificial neural networks (ANNs) that accurately predict both behavior and neural activity in the visual system during object recognition tasks. Unlike dominant search models, ANNs can provide predictions for any image. First we test ANN-based object recognition models with simplified stimuli typically used in studies of visual search. We find these models exhibit a hallmark effect of such studies: a drop in target detection accuracy as the number of distractors increases. Further experiments show this effect results from learned representations: networks that are not pre-trained for object recognition can achieve near perfect accuracy. Next we test these models with complex natural images, using a version of the Pascal VOC dataset where each image has a visual search difficulty score, derived from human reaction times. We find models exhibit a drop in accuracy as search difficulty score increases. We conclude that ANN-based object recognition models account for aspects of visual search behavior across stimuli types, and discuss how to extend these results.</p>
    </body>
    </html>
    